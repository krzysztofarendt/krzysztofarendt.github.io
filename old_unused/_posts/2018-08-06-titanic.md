---
layout: post
title: "Model selection: Titanic data set"
published: true
tags: python, matplotlib, scikit-learn
---

Usually, it's not clear upfront which model type is best for a given problem. Furthermore, many machine learning models rely on different hyperparameters that have to be tuned in order to get satisfactory results. In this post I will compare the performance of several models for the [Titanic](https://www.kaggle.com/c/titanic) data set posted on Kaggle: Decision Tree, Random Forest, k-Nearest Neighbor, and SVM. Although the data set is very small and easy to process, it's a great playground for testing and learning. The original notebook used to produce the results in this post is [here](https://github.com/krzysztofarendt/kaggle-titanic/blob/master/titanic_analysis.ipynb). I used models from [scikit-learn](http://scikit-learn.org).


# Data set and evaluation method

Let's start with a short description of the data set and the task. The task is to predict the survivalibity of the Titanic passengers based on their travel information: ticket class, fare, name, age, sex, family members, cabin, embarkment location and assigned cabin. The learning data set contains 891 entries, however some of them are incomplete (e.g. age is missing in 20% of entries and cabin in 77%).

The data preprocessing procedure adopted in this analysis is as follows:

1. Drop features with too many missing values: cabin
2. Drop features with little influence on the result: ticket number
3. Extract person's title from the name
4. Guess missing values: age based on title, fare based on ticket class
5. Sum columns with family members (SiSp + Parch)
6. Convert all string/categorical variables to discrete numerical (for scikit-learn classifiers)

Let me skip the [code](https://github.com/krzysztofarendt/kaggle-titanic/blob/master/titanic_analysis.ipynb), and simply present some highlights regarding the available data.


![Median age vs. title](/gfx/titanic/median_age_vs_title.png)

**Fig. 1:** Median age vs. title.

![Median fare vs. class](/gfx/titanic/median_fare_vs_class.png)

**Fig. 2:** Median fare vs. class.

The few first lines of the preprocessed data set is as follows:

PassengerId | Survived | Pclass | Sex | Age | Fare   | Embarked | Title | Family
------------|----------|--------|-----|-----|--------|----------|-------|--------
1           | 0        | 3      | 1   | 22.0| 7.2500 |3         | 4     | 1
2           | 1        | 1      | 2   | 38.0| 71.283 |1         | 0     | 1
3           | 1        | 3      | 2   | 26.0| 7.9250 |3         | 1     | 0
4           | 1        | 1      | 2   | 35.0| 53.100 |3         | 0     | 1
5           | 0        | 3      | 1   | 35.0| 8.0500 |3         | 4     | 0

We treat the survival column as the labels (dependent variable), whereas other columns can, but don't have to, be used as the features (independent variables).

All methods are evaluated based on their accuracy, calculated as the ratio of correctly classified survivals [%]. The accuracy distribution is calculated using a 10-fold cross-validation.

# Decision tree vs. Random Forest

First, let's compare the vanilla Decision Tree with the Random Forest. Both models are great for preliminary tests, because there is very little tuning required, especially in the case of the Decision Tree. Since the Random Forest is an ensemble method, its results depend on the number of trees *n* we want to include. In Fig. 3 we can see that the mean accuracy oscilates around 81-82% after passing *n* around 20.

![Random Forest tuning](/gfx/titanic/random_forest_tuning.png)

**Fig. 3:** Random Forest accuracy [%] vs. number of trees *n* (shaded area for mean +/- standard deviation).

Both the Decision Tree and Random Forest provide similar accuracy (Fig. 4), which in this case is likely to the low number of features and observations.

![Decision Tree vs. Random Forest](/gfx/titanic/decision_tree_vs_random_forest.png)

**Fig. 4:** Decision Tree vs. Random Forest accuracy for different feature sets.

# k-Nearest Neighbors

k-NN is a lazy learning algorithm, meaning that the outcome is evaluated only locally, based on the neighboring values. In the classification problem, as this one, the outcome is classified by a majority vote from observations with similar features. The votes can have the same weight (*uniform*) or be scaled by the distance from the observation question. In both cases, the parameter to be tuned is the number of neighbors *k*. The best combinations are marked with an arrow in Fig. 5.

![k-NN](/gfx/titanic/knn.png)

**Fig. 5:** k-Nearest Neighbors accuracy for different number of neighbors *k* and two different feature sets (shaded area for mean +/- standard deviation).

# SVM

The SVM algorithm tries to construct a hyperplane between the categories (here survived or not) that creates the widest possible gap. A [kernel trick](https://en.wikipedia.org/wiki/Kernel_method) can be used for non-linear classification. In this example a radial basis function (RBF) is used as the kernel function. SVM+RBF has two main hyperparameters to be tuned: *C* - the error penalty for incorrectly classified observations, *gamma* - defines how much influence a single observation has. In practice it's unknown what these hyperparameters should be. In addition, one influences the other. In Fig. 6 we can see how non-linear the impact of *C* and *gamma* on the model accuracy is. In this example, it's reasonable to assume *C* somewhere between 1 and 1e6 (!!!), and *gamma* somewhere between 0.001 and 0.1.

![SVM tuning](/gfx/titanic/svm_tuning.png)

**Fig. 6:** SVM accuracy for different values of *C* and *gamma*.

In this example, the more features are taken into account, the more accurate the SVM becomes. It's not always true, because some features may not explain the labels well or may be redundant with respect to other features already included. Also, we can compare Fig. 6 with Fig. 4 to see that the feature selection depends also on the model type. E.g. a simple Decision Tree with just one feature (sex) works almost equally good as a sophisticated SVM with a non-linear kernel function (just 3% worse accuracy). Even for this small data set it takes around 1 hour to find the right values for *C* and *gamma* (using a single core), whereas the construction of a Decision Tree takes a second or less. In addition, we may be interested not only in the accuracy, but also in the interpretability of the model - here again the Decision Tree has a slight advantage.

![SVM features](/gfx/titanic/svm_features.png)

**Fig. 6:** SVM accuracy for different feature sets.